{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f77af7",
   "metadata": {},
   "source": [
    "## Detection code of gravels from large ortho images using a trained model\n",
    "# (update 2024 09 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full window\n",
    "\n",
    "%%HTML\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2d1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Detectron2 and related modules\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data.catalog import DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a74a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detection model\n",
    "MODEL_PATH = \"directory path of your detection model\"\n",
    "model_path = os.path.join(MODEL_PATH, \"model name of your detection model (*.pth)\")\n",
    "\n",
    "# Set up predictor\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) # load backbone\n",
    "cfg.MODEL.WEIGHTS = model_path # load weight of your model\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "cfg.INPUT.MAX_SIZE_TEST = 0\n",
    "cfg.INPUT.MIN_SIZE_TEST = 0\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 500\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23153e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data\n",
    "TARGET_PATH = \"Directory path of your target ortho images\"\n",
    "\n",
    "# set up image catalog\n",
    "from detectron2.data import MetadataCatalog\n",
    "MetadataCatalog.get(\"gravel_meta\").thing_classes = [\"Gravel\"]\n",
    "detector_metadata = MetadataCatalog.get(\"gravel_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec32dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code outputs the detection results as original format of JSON file.\n",
    "# Here, define the json format (basement) and functions\n",
    "\n",
    "class cocojson_initialize:\n",
    "    def __init__(self):\n",
    "        #initialize json data\n",
    "        licenses = {\n",
    "                    \"name\":\"\",\n",
    "                    \"id\":0,\n",
    "                    \"url\":\"\"\n",
    "                    }\n",
    "        info     = {\n",
    "                    \"contributor\":\"\",\n",
    "                    \"date_created\":\"\",\n",
    "                    \"description\":\"\",\n",
    "                    \"url\":\"\",\n",
    "                    \"version\":\"\",\n",
    "                    \"year\":\"\",\n",
    "                    } \n",
    "        categories  = []\n",
    "        n_ct = 0\n",
    "        for class_name in detector_metadata.thing_classes:\n",
    "            n_ct += 1\n",
    "            add_cat = {\n",
    "                       'id': n_ct,\n",
    "                       'name':class_name,\n",
    "                       'supercategory':'none',\n",
    "                       }\n",
    "            categories.append(add_cat)  \n",
    "        images      = []\n",
    "        annotations = []\n",
    "        \n",
    "        self.jData  = {\n",
    "                       \"licenses\":[licenses],\n",
    "                       \"info\":info,\n",
    "                       \"categories\":categories, \n",
    "                       \"images\":images,\n",
    "                       \"annotations\":annotations  \n",
    "                       }\n",
    "def PolyArea(seg):\n",
    "    if len(seg)>=2:\n",
    "        x = seg[0::2]\n",
    "        y = seg[1::2]\n",
    "        return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def Mask2Seg(mask, shift):\n",
    "    mask = mask.astype(np.uint8)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)#cv2.CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_TC89_L1\n",
    "    if contours:\n",
    "        boundaries = np.reshape(contours[0],(1,-1))[0]\n",
    "        #calc position in the global image\n",
    "        x = boundaries[0::2]\n",
    "        y = boundaries[1::2]\n",
    "        boundaries[0::2] = [n + shift[0] for n in x]\n",
    "        boundaries[1::2] = [n + shift[1] for n in y]\n",
    "        \n",
    "        boundaries = boundaries.tolist()\n",
    "        #boundaries = str(boundaries[0])\n",
    "        #boundaries.replace( '\\n' , '' )\n",
    "    else:\n",
    "        boundaries = []\n",
    "    return boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect boulders from Ortho images\n",
    "\n",
    "# Load modules\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "import xml.etree.ElementTree as ET\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import glob, json\n",
    "from tqdm.notebook import tqdm\n",
    "from tifffile import TiffFile\n",
    "\n",
    "# Make up output directory\n",
    "out_dir_name = 'outputs'\n",
    "os.makedirs(os.path.join(TARGET_PATH,out_dir_name), exist_ok=True)\n",
    "\n",
    "# Set up detection settings\n",
    "pix_step_rate = [0.25, 0.25]\n",
    "pix_window    = [2000, 2000]\n",
    "apply_sharp   = False\n",
    "\n",
    "# Calc scan settinsg\n",
    "pix_step = np.round(np.multiply(pix_step_rate, pix_window))\n",
    "pix_step = np.asarray(pix_step, dtype = int)\n",
    "pix_step = pix_step.tolist()\n",
    "print('Scan size:' + str(pix_window))\n",
    "print('Scan steps:' + str(pix_step))\n",
    "\n",
    "# Get target image list \n",
    "types = ['*.tif','*.jpg','*.JPG']\n",
    "tag_list = []\n",
    "for im_type in types:\n",
    "    tag_list = tag_list + glob.glob(os.path.join(TARGET_PATH,im_type))\n",
    "\n",
    "num_image = len(tag_list)\n",
    "print(\"Count of images:\" + str(num_image))\n",
    "\n",
    "# Appluy detections to each image\n",
    "n_im = 0\n",
    "print('Total progress:')\n",
    "for d in tqdm(tag_list):\n",
    "    # Initialize\n",
    "    n_im += 1\n",
    "    imname = os.path.splitext(os.path.basename(d))\n",
    "    SAVE_DIR = os.path.join(TARGET_PATH, out_dir_name)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    jData = cocojson_initialize()\n",
    "    \n",
    "    # Show progress\n",
    "    print('['+str(n_im)+ '/' + str(len(tag_list))+']:'+imname[0])\n",
    "        \n",
    "    # Load ortho image as a memory map\n",
    "    print(\"Loading image as memory map...\",end=\"\")\n",
    "    with TiffFile(d) as tif:\n",
    "        mmap = tif.asarray(out=\"memmap\") \n",
    "    print(\"Done\")\n",
    "\n",
    "    # Calc split size\n",
    "    pix_im  = mmap.shape\n",
    "    num_col = math.ceil((pix_im[1]-pix_window[1])/pix_step[1])+1\n",
    "    num_row = math.ceil((pix_im[0]-pix_window[0])/pix_step[0])+1\n",
    "    \n",
    "    # Get split locations\n",
    "    imageGridData = []\n",
    "    for c in tqdm(range(num_col), leave=False):\n",
    "        for r in tqdm(range(num_row), leave=False):\n",
    "            row_range   = [pix_step[0]*r, pix_step[0]*r+pix_window[0]]\n",
    "            col_range   = [pix_step[1]*c, pix_step[1]*c+pix_window[1]]\n",
    "            imageGridData.append([row_range[0], row_range[1],col_range[0], col_range[1]])\n",
    "\n",
    "    # Get output image info        \n",
    "    im_data  = {\n",
    "                \"id\":1,\n",
    "                \"width\":pix_im [1],\n",
    "                \"height\":pix_im[0],\n",
    "                \"file_name\":imname[0]+imname[1],\n",
    "                \"license\":0,\n",
    "                \"flickr_url\":\"\",\n",
    "                \"coco_url\":\"\",\n",
    "                \"date_capture\":\"\",\n",
    "                \"window_size\":pix_window,\n",
    "                \"window_step_rate\":pix_step_rate,\n",
    "                \"image_grid\":imageGridData\n",
    "                }\n",
    "    \n",
    "    # Add image info into the output jData\n",
    "    jData.jData[\"images\"].append(im_data)\n",
    "    \n",
    "    # Apply detections\n",
    "    an_id = 0\n",
    "    for c in tqdm(range(num_col), leave=False):\n",
    "        for r in tqdm(range(num_row), leave=False):\n",
    "            # Split ortho image\n",
    "            row_range   = [pix_step[0]*r, pix_step[0]*r+pix_window[0]]\n",
    "            col_range   = [pix_step[1]*c, pix_step[1]*c+pix_window[1]]\n",
    "            im_temp     = np.array(mmap[row_range[0]:row_range[1],col_range[0]:col_range[1],:])\n",
    "            \n",
    "            # Apply sharpness to the split image (optional)\n",
    "            if apply_sharp:\n",
    "                kernel = np.array([[0, -1, 0],[-1, 5,-1], [0, -1, 0]])\n",
    "                im_temp = cv2.filter2D(im_temp, -1, kernel)\n",
    "            \n",
    "            # Remove alpha layer from tiff\n",
    "            if im_temp.shape[2]==4:\n",
    "                im_temp = im_temp[:,:, 0:3]#if tif\n",
    "\n",
    "            # Convert RGB to BGR\n",
    "            im_temp = im_temp[:, :, ::-1]\n",
    "            pix_im_temp = im_temp.shape\n",
    "\n",
    "            # Detect\n",
    "            outputs = predictor(im_temp)\n",
    "\n",
    "            # Extract data from the detected raw result\n",
    "            bboxes     = outputs[\"instances\"].pred_boxes.tensor.to(\"cpu\").numpy()\n",
    "            scores     = outputs[\"instances\"].scores.to(\"cpu\").numpy()\n",
    "            classes    = outputs[\"instances\"].pred_classes\n",
    "            num_ins    = len(bboxes)\n",
    "            mask_array = outputs[\"instances\"].pred_masks.to(\"cpu\").numpy()\n",
    "            \n",
    "            # Get label and counter of each instance\n",
    "            for idx in range(num_ins):\n",
    "                an_id += 1\n",
    "                # Get label name\n",
    "                label_name = detector_metadata.thing_classes[classes[idx].item()]\n",
    "                                                    \n",
    "                # Get boundarybox\n",
    "                bbox   = [bboxes[idx][0] + col_range[0], bboxes[idx][1] + row_range[0], bboxes[idx][2] - bboxes[idx][0] + 1, bboxes[idx][3] - bboxes[idx][1] + 1]\n",
    "                \n",
    "                # Convert mask from BW image to mask lines(xy)\n",
    "                boundary = Mask2Seg(mask_array [idx], [col_range[0], row_range[0]])\n",
    "                \n",
    "                # Mmake annotations info\n",
    "                an_data    = {\"id\":an_id,\n",
    "                              \"image_id\":1,\n",
    "                              \"category_id\":classes[idx].item()+1,\n",
    "                              \"segmentation\":[boundary],\n",
    "                              \"area\":PolyArea(boundary),\n",
    "                              \"bbox\":bbox,\n",
    "                              \"iscrowd\":0,\n",
    "                              \"attributes\":{\"occluded\":0},\n",
    "                              \"score\":str(scores[idx]),\n",
    "                              }\n",
    "\n",
    "                # Add annotation info into output jData\n",
    "                jData.jData[\"annotations\"].append(an_data)\n",
    "            \n",
    "            # Save annoated split image \n",
    "            save_annotated_im = False\n",
    "            if save_annotated_im:\n",
    "                v = Visualizer(im_temp,\n",
    "                               metadata=detector_metadata, \n",
    "                               scale=1.0\n",
    "                              )\n",
    "                out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "                im_labeled = cv2.resize(out.get_image(), dsize=(pix_im_temp[1], pix_im_temp[0]))\n",
    "                cv2.imwrite(os.path.join(SAVE_DIR, new_imname + '_labeled' + '.jpg'),im_labeled)\n",
    "                                                        \n",
    "    # Export jData as json\n",
    "    with open (os.path.join(SAVE_DIR,imname[0]+'_detect_object.json'), 'w') as fp:\n",
    "        json.dump(jData.jData, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
